<workflow-app name="orcid_to_result_from_semrel_propagation" xmlns="uri:oozie:workflow:0.5">
<parameters>
    <property>
        <name>sourcePath</name>
        <description>the source path</description>
    </property>
    <property>
        <name>allowedsemrels</name>
        <description>the semantic relationships allowed for propagation</description>
    </property>
<!--    <property>-->
<!--        <name>sparkDriverMemory</name>-->
<!--        <description>memory for driver process</description>-->
<!--    </property>-->
<!--    <property>-->
<!--        <name>sparkExecutorMemory</name>-->
<!--        <description>memory for individual executor</description>-->
<!--    </property>-->
<!--    <property>-->
<!--        <name>sparkExecutorCores</name>-->
<!--        <description>number of cores used by single executor</description>-->
<!--    </property>-->
    <property>
        <name>writeUpdate</name>
        <description>writes the information found for the update. No double check done if the information is already present</description>
    </property>
    <property>
        <name>saveGraph</name>
        <description>writes new version of the graph after the propagation step</description>
    </property>
</parameters>

<start to="OrcidToResultFromSemRelPropagation"/>

<kill name="Kill">
    <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
</kill>

<action name="OrcidToResultFromSemRelPropagation">
    <spark xmlns="uri:oozie:spark-action:0.2">
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <master>yarn-cluster</master>
        <mode>cluster</mode>
        <name>OrcidToResultFromSemRelPropagation</name>
        <class>eu.dnetlib.dhp.orcidtoresultfromsemrel.SparkOrcidToResultFromSemRelJob</class>
        <jar>dhp-propagation-${projectVersion}.jar</jar>
        <spark-opts>
            --num-executors=${sparkExecutorNumber}
            --executor-memory=${sparkExecutorMemory}
            --executor-cores=${sparkExecutorCores}
            --driver-memory=${sparkDriverMemory}
            --conf spark.extraListeners=${spark2ExtraListeners}
            --conf spark.sql.queryExecutionListeners=${spark2SqlQueryExecutionListeners}
            --conf spark.yarn.historyServer.address=${spark2YarnHistoryServerAddress}
            --conf spark.eventLog.dir=${nameNode}${spark2EventLogDir}
            --conf spark.dynamicAllocation.enabled=true
            --conf spark.dynamicAllocation.maxExecutors=${spark2MaxExecutors}
        </spark-opts>
        <arg>-mt</arg> <arg>yarn-cluster</arg>
        <arg>--sourcePath</arg><arg>${sourcePath}</arg>
        <arg>--allowedsemrels</arg><arg>${allowedsemrels}</arg>
        <arg>--writeUpdate</arg><arg>${writeUpdate}</arg>
        <arg>--saveGraph</arg><arg>${saveGraph}</arg>
    </spark>
    <ok to="End"/>
    <error to="Kill"/>
</action>

<end name="End"/>
</workflow-app>