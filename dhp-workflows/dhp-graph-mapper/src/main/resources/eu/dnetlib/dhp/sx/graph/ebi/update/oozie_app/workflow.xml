    <workflow-app name="Create EBI Dataset" xmlns="uri:oozie:workflow:0.5">
        <parameters>
            <property>
                <name>sourcePath</name>
                <description>the Working Path</description>
            </property>
            <property>
                <name>workingPath</name>
                <description>the Working Path</description>
            </property>
            <property>
                <name>sparkDriverMemory</name>
                <description>memory for driver process</description>
            </property>
            <property>
                <name>sparkExecutorMemory</name>
                <description>memory for individual executor</description>
            </property>
            <property>
                <name>sparkExecutorCores</name>
                <description>number of cores used by single executor</description>
            </property>
        </parameters>

        <start to="DownloadEBILinks"/>


        <kill name="Kill">
            <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
        </kill>


        <action name="DownloadEBILinks">
            <spark xmlns="uri:oozie:spark-action:0.2">
                <master>yarn-cluster</master>
                <mode>cluster</mode>
                <name>Incremental Download EBI Links</name>
                <class>eu.dnetlib.dhp.sx.graph.ebi.SparkDownloadEBILinks</class>
                <jar>dhp-graph-mapper-${projectVersion}.jar</jar>
                <spark-opts>
                    --executor-memory=${sparkExecutorMemory}
                    --executor-cores=${sparkExecutorCores}
                    --driver-memory=${sparkDriverMemory}
                    --conf spark.extraListeners=${spark2ExtraListeners}
                    --conf spark.sql.shuffle.partitions=2000
                    --conf spark.sql.queryExecutionListeners=${spark2SqlQueryExecutionListeners}
                    --conf spark.yarn.historyServer.address=${spark2YarnHistoryServerAddress}
                    --conf spark.eventLog.dir=${nameNode}${spark2EventLogDir}
                </spark-opts>
                <arg>--sourcePath</arg><arg>${sourcePath}</arg>
                <arg>--workingPath</arg><arg>${workingPath}</arg>
                <arg>--master</arg><arg>yarn</arg>
            </spark>
            <ok to="OverrideFolders"/>
            <error to="Kill"/>
        </action>
        <action name="OverrideFolders">
            <fs>
                <delete path="${sourcePath}/ebi_links_dataset_old"/>
                <move source="${sourcePath}/ebi_links_dataset" target="${sourcePath}/ebi_links_dataset_old"/>
                <move source="${workingPath}/links_final" target="${sourcePath}/ebi_links_dataset"/>
            </fs>
            <ok to="End"/>
            <error to="Kill"/>
        </action>
        <end name="End"/>
    </workflow-app>